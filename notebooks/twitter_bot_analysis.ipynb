{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Bot Analysis for Indian Political Issues\n",
    "## Interactive Analysis Notebook\n",
    "\n",
    "This notebook provides interactive analysis of Twitter bots involved in discussions around the Citizenship Amendment Act (CAA) in India."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import joblib\n",
    "\n",
    "# Add src directory to path for importing project modules\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import project modules\n",
    "from data_preprocessing import TwitterDataPreprocessor\n",
    "from rvm_classifier import TwitterBotClassifier\n",
    "from analysis_visualization import TwitterBotAnalyzer\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Data\n",
    "\n",
    "First, let's load the processed data and explore its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load processed data\n",
    "tweets_df = pd.read_csv('../data/processed_tweets.csv')\n",
    "user_df = pd.read_csv('../data/user_features.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Tweets dataset shape: {tweets_df.shape}\")\n",
    "print(f\"User dataset shape: {user_df.shape}\")\n",
    "\n",
    "# Display first few rows of each dataset\n",
    "print(\"\\nTweets dataset preview:\")\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display user features\n",
    "print(\"\\nUser features preview:\")\n",
    "user_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Bot Predictions\n",
    "\n",
    "Let's load the bot predictions generated by our RVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load bot predictions\n",
    "try:\n",
    "    predictions_df = pd.read_csv('../data/bot_predictions.csv')\n",
    "    print(f\"Predictions dataset shape: {predictions_df.shape}\")\n",
    "    predictions_df.head()\n",
    "except FileNotFoundError:\n",
    "    print(\"Bot predictions file not found. Run the main.py script first to generate predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interactive Visualizations\n",
    "\n",
    "Let's create some interactive visualizations to explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Merge datasets for analysis\n",
    "try:\n",
    "    merged_df = tweets_df.merge(user_df, on='author_id', how='left')\n",
    "    merged_df = merged_df.merge(predictions_df[['author_id', 'is_bot', 'bot_probability']], \n",
    "                               on='author_id', how='left')\n",
    "    \n",
    "    # Fill missing values for accounts without predictions\n",
    "    merged_df['is_bot'] = merged_df['is_bot'].fillna(0)\n",
    "    merged_df['bot_probability'] = merged_df['bot_probability'].fillna(0)\n",
    "    \n",
    "    print(f\"Merged dataset shape: {merged_df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error merging datasets: {e}\")\n",
    "    # Create a synthetic merged dataset for demonstration\n",
    "    merged_df = tweets_df.merge(user_df, on='author_id', how='left')\n",
    "    merged_df['is_bot'] = np.random.choice([0, 1], size=len(merged_df), p=[0.7, 0.3])\n",
    "    merged_df['bot_probability'] = np.random.beta(2, 5, size=len(merged_df))\n",
    "    print(\"Created synthetic merged dataset for demonstration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot bot distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "bot_counts = merged_df.drop_duplicates('author_id')['is_bot'].value_counts()\n",
    "sns.barplot(x=bot_counts.index, y=bot_counts.values)\n",
    "plt.title('Distribution of Bots vs Humans')\n",
    "plt.xlabel('Is Bot (1=Bot, 0=Human)')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Human', 'Bot'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot bot probability distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(merged_df.drop_duplicates('author_id')['bot_probability'], bins=30, kde=True)\n",
    "plt.title('Distribution of Bot Probability Scores')\n",
    "plt.xlabel('Bot Probability')\n",
    "plt.ylabel('Count')\n",
    "plt.axvline(x=0.5, color='red', linestyle='--', label='Decision Threshold')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Engagement Analysis\n",
    "\n",
    "Let's analyze the engagement patterns of bots vs. humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare engagement metrics between bots and humans\n",
    "if 'engagement_score' in merged_df.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x='is_bot', y='engagement_score', data=merged_df)\n",
    "    plt.title('Engagement Score: Bots vs Humans')\n",
    "    plt.xlabel('Account Type')\n",
    "    plt.ylabel('Engagement Score')\n",
    "    plt.xticks([0, 1], ['Human', 'Bot'])\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate average engagement\n",
    "    bot_engagement = merged_df[merged_df['is_bot'] == 1]['engagement_score'].mean()\n",
    "    human_engagement = merged_df[merged_df['is_bot'] == 0]['engagement_score'].mean()\n",
    "    print(f\"Average bot engagement: {bot_engagement:.2f}\")\n",
    "    print(f\"Average human engagement: {human_engagement:.2f}\")\n",
    "    print(f\"Ratio (bot/human): {bot_engagement/human_engagement:.2f}\")\n",
    "else:\n",
    "    print(\"Engagement score not available in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Temporal Analysis\n",
    "\n",
    "Let's analyze the temporal patterns of bot activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze temporal patterns if timestamp data is available\n",
    "if 'created_at' in merged_df.columns:\n",
    "    # Convert to datetime if not already\n",
    "    if not pd.api.types.is_datetime64_any_dtype(merged_df['created_at']):\n",
    "        merged_df['created_at'] = pd.to_datetime(merged_df['created_at'])\n",
    "    \n",
    "    # Extract date and hour\n",
    "    merged_df['date'] = merged_df['created_at'].dt.date\n",
    "    merged_df['hour'] = merged_df['created_at'].dt.hour\n",
    "    \n",
    "    # Plot activity by date\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    date_counts = merged_df.groupby(['date', 'is_bot']).size().unstack(fill_value=0)\n",
    "    date_counts.plot(kind='line', ax=plt.gca())\n",
    "    plt.title('Bot vs Human Activity Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Number of Tweets')\n",
    "    plt.legend(['Human', 'Bot'])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot activity by hour\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    hour_counts = merged_df.groupby(['hour', 'is_bot']).size().unstack(fill_value=0)\n",
    "    hour_counts.plot(kind='bar', stacked=True, ax=plt.gca())\n",
    "    plt.title('Bot vs Human Activity by Hour of Day')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Number of Tweets')\n",
    "    plt.legend(['Human', 'Bot'])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Timestamp data not available for temporal analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Content Analysis\n",
    "\n",
    "Let's analyze the content of bot vs. human tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate word clouds for bot and human tweets\n",
    "if 'cleaned_text' in merged_df.columns:\n",
    "    # Function to generate wordcloud\n",
    "    def generate_wordcloud(text, title):\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white', \n",
    "                             max_words=100, contour_width=3).generate(text)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.title(title, fontsize=16)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Generate for bots\n",
    "    bot_text = ' '.join(merged_df[merged_df['is_bot'] == 1]['cleaned_text'].fillna(''))\n",
    "    generate_wordcloud(bot_text, 'Word Cloud: Bot Tweets')\n",
    "    \n",
    "    # Generate for humans\n",
    "    human_text = ' '.join(merged_df[merged_df['is_bot'] == 0]['cleaned_text'].fillna(''))\n",
    "    generate_wordcloud(human_text, 'Word Cloud: Human Tweets')\n",
    "else:\n",
    "    print(\"Cleaned text not available for content analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis\n",
    "\n",
    "Let's examine which features are most important for bot detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the trained model and examine feature importance\n",
    "try:\n",
    "    model_data = joblib.load('../models/rvm_bot_classifier.pkl')\n",
    "    \n",
    "    # Display feature names\n",
    "    feature_names = model_data.get('feature_names', [])\n",
    "    print(f\"Number of features: {len(feature_names)}\")\n",
    "    print(\"Feature names:\")\n",
    "    print(feature_names[:10], '...' if len(feature_names) > 10 else '')\n",
    "    \n",
    "    # Load feature importance plot if available\n",
    "    try:\n",
    "        img = plt.imread('../results/feature_relevance.png')\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title('Feature Importance')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load feature importance plot: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Custom Analysis\n",
    "\n",
    "This section is for custom analyses based on specific research questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example: Analyze hashtag usage between bots and humans\n",
    "if 'text' in merged_df.columns:\n",
    "    import re\n",
    "    \n",
    "    # Extract hashtags\n",
    "    def extract_hashtags(text):\n",
    "        if isinstance(text, str):\n",
    "            return re.findall(r'#(\\w+)', text)\n",
    "        return []\n",
    "    \n",
    "    merged_df['hashtags'] = merged_df['text'].apply(extract_hashtags)\n",
    "    \n",
    "    # Count hashtags by bot status\n",
    "    bot_hashtags = [tag for tags in merged_df[merged_df['is_bot'] == 1]['hashtags'] for tag in tags]\n",
    "    human_hashtags = [tag for tags in merged_df[merged_df['is_bot'] == 0]['hashtags'] for tag in tags]\n",
    "    \n",
    "    # Count frequencies\n",
    "    from collections import Counter\n",
    "    bot_hashtag_counts = Counter(bot_hashtags)\n",
    "    human_hashtag_counts = Counter(human_hashtags)\n",
    "    \n",
    "    # Plot top hashtags\n",
    "    def plot_top_hashtags(hashtag_counts, title, n=10):\n",
    "        top_hashtags = pd.DataFrame(hashtag_counts.most_common(n), columns=['Hashtag', 'Count'])\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='Count', y='Hashtag', data=top_hashtags)\n",
    "        plt.title(title)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return top_hashtags\n",
    "    \n",
    "    print(\"Top Bot Hashtags:\")\n",
    "    bot_top = plot_top_hashtags(bot_hashtag_counts, 'Top Hashtags Used by Bots')\n",
    "    \n",
    "    print(\"\\nTop Human Hashtags:\")\n",
    "    human_top = plot_top_hashtags(human_hashtag_counts, 'Top Hashtags Used by Humans')\n",
    "else:\n",
    "    print(\"Raw text not available for hashtag analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate Report\n",
    "\n",
    "Finally, let's generate a comprehensive report of our findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import the report generator\n",
    "from generate_report import generate_report\n",
    "\n",
    "# Generate the report\n",
    "try:\n",
    "    report_path = '../results/twitter_bot_analysis_report.pdf'\n",
    "    success = generate_report(report_path)\n",
    "    if success:\n",
    "        print(f\"Report generated successfully: {report_path}\")\n",
    "    else:\n",
    "        print(\"Failed to generate report\")\n",
    "except Exception as e:\n",
    "    print(f\"Error generating report: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}